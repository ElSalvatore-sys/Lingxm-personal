#!/usr/bin/env python3
"""
Generate Arabic C1-C2 sentence file.
Extract 2 existing examples + add placeholders for Claude Code to fill.
"""

import json
from datetime import date

# Load vocabulary
with open('public/data/hassan/ar.json', 'r', encoding='utf-8') as f:
    vocab = json.load(f)

print(f"Loaded {len(vocab)} Arabic words")

# Prepare output
output = {
    "metadata": {
        "language": "ar",
        "language_name": "Arabic",
        "level": "C1-C2",
        "source_profiles": ["hassan"],
        "source_files": ["public/data/hassan/ar.json"],
        "total_words": len(vocab),
        "total_sentences": len(vocab) * 3,
        "generated_date": str(date.today()),
        "version": "1.0",
        "generator": "Claude Code",
        "domain": "professional, business, advanced discourse",
        "translation_languages": ["en", "ar"],
        "notes": "Generated from Hassan's C1-C2 Arabic vocabulary. Professional and business contexts for advanced learners."
    },
    "sentences": {}
}

# Process each word - extract existing examples
for idx, word_data in enumerate(vocab, 1):
    word = word_data['word']
    examples = word_data.get('examples', {})

    sentences = []

    # Extract EN example (sentence 1)
    if 'en' in examples and len(examples['en']) == 2:
        sentences.append({
            "id": f"ar_{idx:03d}_001",
            "sentence": examples['en'][0],
            "translation": examples['en'][1],
            "translation_language": "en",
            "target_word": word,
            "target_index": -1,
            "difficulty": "basic",
            "domain": "professional"
        })

    # Extract AR example (sentence 2)
    if 'ar' in examples and len(examples['ar']) == 2:
        sentences.append({
            "id": f"ar_{idx:03d}_002",
            "sentence": examples['ar'][0],
            "translation": examples['ar'][1],
            "translation_language": "ar",
            "target_word": word,
            "target_index": -1,
            "difficulty": "intermediate",
            "domain": "professional"
        })

    # Placeholder for sentence 3 (to be generated by Claude Code)
    sentences.append({
        "id": f"ar_{idx:03d}_003",
        "sentence": f"[TO_GENERATE]",
        "translation": f"[TO_GENERATE]",
        "translation_language": "en",
        "target_word": word,
        "target_index": -1,
        "difficulty": "advanced",
        "domain": "professional",
        "_word_data": {
            "word": word,
            "translations": word_data.get('translations', {}),
            "explanation": word_data.get('explanation', {})
        }
    })

    output['sentences'][word] = sentences

# Save template
with open('temp/ar-sentences-template.json', 'w', encoding='utf-8') as f:
    json.dump(output, f, ensure_ascii=False, indent=2)

print(f"✅ Created temp/ar-sentences-template.json")
print(f"   {len(vocab)} words × 3 sentences = {len(vocab) * 3} total")
print(f"   {len(vocab) * 2} extracted, {len(vocab)} to generate")

# Create a simple list of words for generation reference
word_list = []
for idx, word_data in enumerate(vocab, 1):
    word_list.append({
        "id": f"ar_{idx:03d}",
        "word": word_data['word'],
        "translations": word_data.get('translations', {}),
        "explanation": word_data.get('explanation', {}),
        "examples": word_data.get('examples', {})
    })

with open('temp/ar-words-for-generation.json', 'w', encoding='utf-8') as f:
    json.dump(word_list, f, ensure_ascii=False, indent=2)

print(f"✅ Created temp/ar-words-for-generation.json for reference")
